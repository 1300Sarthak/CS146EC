1. Other than speed some of the things you have to look into when you are setting to analyze the algorithm, are the amount of memory, CPU power, storage, network power, etc. the algorithm takes up (space complexity), the scalability of the program, and how well you can scale it up or down depending on the data sizes or the task given. Along with that most importantly the reliability and accuracy of the program make sure that it solves everything in a way that is correct and with robustness. Finally, the last thing you look at is how maintainable the program is, can other people be given the code and understand what's going on quickly and efficiently?

2. If we look at BST (Binary Search Tree) for our data structure we can see that there are multiple strengths and weaknesses. For starters looking at strengths, BST is very efficient in its way to search as its Big O notation is O(log n) for all operations, making it much more speed efficient than other data structures such as arrays. Along with that BST is ordered very well as it keeps everything in order in how it's being sorted. But that's where the benefits end, the downside for BST is that its space requirements are a lot more than arrays, even ones that are very full and large data sets. Not to mention that the maintenance required for it is a lot. Causing it to somewhat be clunky.